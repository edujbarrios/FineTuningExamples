
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning de Llama 3.2 Vision para clasificación de imágenes: Ejemplo funcional\n",
    "\n",
    "Este cuaderno utiliza el modelo Llama 3.2 Vision y realiza un fine-tuning en un dataset público\n",
    "llamado \"cats_vs_dogs\", disponible en Hugging Face. El objetivo es clasificar correctamente las\n",
    "imágenes en dos categorías: \"cats\" y \"dogs\". También incluye una evaluación para justificar su correcto funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth transformers datasets torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from unsloth import VisionFineTuner\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama-3.2-vision\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset \"cats_vs_dogs\" está disponible en Hugging Face y ya está dividido en imágenes etiquetadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cats_vs_dogs\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos las imágenes y etiquetas al formato necesario para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def preprocess(example):\n",
    "    example['image'] = preprocess_image(example['image'])\n",
    "    example['text'] = tokenizer(example['label'], truncation=True, padding=\"max_length\", max_length=16)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos los parámetros para el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner = VisionFineTuner(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    learning_rate=3e-5,\n",
    "    batch_size=8,\n",
    "    epochs=3,\n",
    "    output_dir=\"./llama3.2_finetuned_cats_vs_dogs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner.save_model(\"./llama3.2_finetuned_cats_vs_dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una evaluación en un subconjunto del dataset para comprobar la precisión del modelo ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"cats_vs_dogs\", split=\"train[1000:1100]\").map(preprocess)\n",
    "eval_results = finetuner.evaluate(eval_dataset)\n",
    "print(\"Resultados de la evaluación:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar algunas imágenes junto con las predicciones del modelo para verificar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset, model, tokenizer):\n",
    "    model.eval()\n",
    "    for sample in dataset.shuffle().select(range(5)):\n",
    "        image = sample['image']\n",
    "        label = sample['label']\n",
    "        prediction = model(image.unsqueeze(0).to(device))\n",
    "        predicted_label = tokenizer.decode(prediction.argmax(dim=1))\n",
    "        plt.imshow(image.permute(1, 2, 0))\n",
    "        plt.title(f\"Real: {label} | Predicción: {predicted_label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "show_predictions(eval_dataset, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
